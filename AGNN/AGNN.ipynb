{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.stats import kendalltau\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_BA=np.load(\"adj_BA.npy\")\n",
    "adj_BA = torch.FloatTensor(adj_BA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def load_data(path=\"./cora/\", dataset=\"cora\"):\n",
    "    \"\"\"读取引文网络数据cora\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str)) # 使用numpy读取.txt文件\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32) # 获取特征矩阵\n",
    "    labels = encode_onehot(idx_features_labels[:, -1]) # 获取标签\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0])\n",
    "    #print(idx)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    #print(idx_map)\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.dtype(str))\n",
    "    #print(edges_unordered)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten()))\n",
    "                     ).reshape(edges_unordered.shape)\n",
    "    #print(np.where(edges==None))\n",
    "    edges=np.delete(edges,[ 231,  493,  662, 1167, 1455, 2387, 2975, 3049, 3050, 3904, 3905,\n",
    "       3907, 4177, 4428, 4527, 4529, 4545],axis=0)\n",
    "    #print(np.where(edges==None))\n",
    "    np.set_printoptions(threshold=np.inf) \n",
    "    #print(edges)\n",
    "    #edges=edges.int32()\n",
    "    edges=edges.astype(np.int32)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]))\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "   # adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj,edges, features, labels, idx_train, idx_val, idx_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_cora, edges,features, labels, idx_train, idx_val, idx_test=load_data(path=\"./cora/\",dataset=\"cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "adata = sc.read(\"./jazz/jazz.mtx\")\n",
    "\n",
    "data = adata.X\n",
    "adj_jazz = torch.FloatTensor(np.array(data.todense()))\n",
    "adj_jazz=np.where(adj_jazz>0,1,adj_jazz)\n",
    "adj_jazz=torch.FloatTensor(adj_jazz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges=np.genfromtxt(\"./email-univ/email-univ.edges\",\n",
    "                                        dtype=np.int)\n",
    "edges=edges-1\n",
    "print(edges.shape)\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(1133, 1133))\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "adj_email = torch.FloatTensor(np.array(adj.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge=np.genfromtxt(\"./email-dnc/email-dnc.edges\",delimiter=\",\",\n",
    "                                        dtype=np.int)\n",
    "edge=edge-1\n",
    "print(edge)\n",
    "edges=edge[:,:2]\n",
    "print(edges)\n",
    "print(edges.min())\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(2029, 2029))\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "adj_email_2 = torch.FloatTensor(np.array(adj.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge=np.genfromtxt(\"./moreno_oz/out.moreno_oz_oz\",delimiter=\" \",\n",
    "                                        dtype=np.int)\n",
    "edge=edge-1\n",
    "print(edge)\n",
    "edges=edge[:,:2]\n",
    "print(edges)\n",
    "print(edges.max())\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(217, 217))\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "adj_oz = torch.FloatTensor(np.array(adj.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge=np.genfromtxt(\"./opsahl-usairport/out.opsahl-usairport\",delimiter=\" \",\n",
    "                                        dtype=np.int)\n",
    "edge=edge-1\n",
    "print(edge)\n",
    "edges=edge[:,:2]\n",
    "print(edges)\n",
    "print(edges.max())\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(1574, 1574))\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "adj_usa = torch.FloatTensor(np.array(adj.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge=np.genfromtxt(\"./moreno_names/out.moreno_names_names\",delimiter=\" \",\n",
    "                                        dtype=np.int)\n",
    "edge=edge-1\n",
    "\n",
    "edges=edge[:,:2]\n",
    "\n",
    "\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(1773, 1773))\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "adj_bible  = torch.FloatTensor(np.array(adj.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges=np.genfromtxt(\"./librec-filmtrust-trust/out.librec-filmtrust-trust\",\n",
    "                                        dtype=np.int)\n",
    "edges=edges-1\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(874, 874))\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "adj_ftrust = torch.FloatTensor(np.array(adj.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges=np.genfromtxt(\"./pajek-erdos/out.pajek-erdos\",\n",
    "                                        dtype=np.int)\n",
    "edges=edges-1\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(6927, 6927))\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "adj_erdos = torch.FloatTensor(np.array(adj.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges=np.genfromtxt(\"./ego-twitter/out.ego-twitter\",\n",
    "                                        dtype=np.int)\n",
    "edges=edges-1\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(23370, 23370))\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "adj_twitter = torch.FloatTensor(np.array(adj.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges=np.genfromtxt(\"./maayan-Stelzl/out.maayan-Stelzl\",\n",
    "                                        dtype=np.int)\n",
    "edges=edges-1\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(1706, 1706))\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "adj_Stelzl = torch.FloatTensor(np.array(adj.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges=np.genfromtxt(\"./maayan-figeys/out.maayan-figeys\",\n",
    "                                        dtype=np.int)\n",
    "edges=edges-1\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(2239, 2239))\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "adj_figeys = torch.FloatTensor(np.array(adj.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges=np.genfromtxt(\"./maayan-vidal/out.maayan-vidal\",\n",
    "                                        dtype=np.int)\n",
    "edges=edges-1\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(3133, 3133))\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "adj_vidal = torch.FloatTensor(np.array(adj.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges=np.genfromtxt(\"./opsahl-powergrid/out.opsahl-powergrid\",\n",
    "                                        dtype=np.int)\n",
    "edges=edges-1\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(4941, 4941))\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "adj_powergrid = torch.FloatTensor(np.array(adj.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges=np.genfromtxt(\"./arenas-meta/out.arenas-meta\",\n",
    "                                        dtype=np.int)\n",
    "edges=edges-1\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(453, 453))\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "adj_Caenor = torch.FloatTensor(np.array(adj.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "程序主要功能\n",
    "输入：网络图邻接矩阵，需要被设置为感染源的节点序列，感染率，免疫率，迭代次数step\n",
    "输出：被设置为感染源的节点序列的SIR感染情况---每次的迭代结果（I+R）/n\n",
    "'''\n",
    "\n",
    "\n",
    "def update_node_status(graph, node, beta, gamma):\n",
    "    \"\"\"\n",
    "    更新节点状态\n",
    "    :param graph: 网络图\n",
    "    :param node: 节点序数\n",
    "    :param beta: 感染率\n",
    "    :param gamma: 免疫率\n",
    "    \"\"\"\n",
    "    # 如果当前节点状态为 感染者(I) 有概率gamma变为 免疫者(R)\n",
    "   \n",
    "    # 如果当前节点状态为 易感染者(S) 有概率beta变为 感染者(I)\n",
    "    if graph.nodes[node]['status'] == 'S':\n",
    "        # 获取当前节点的邻居节点\n",
    "        # 无向图：G.neighbors(node)\n",
    "        # 有向图：G.predecessors(node)，前驱邻居节点，即指向该节点的节点；G.successors(node)，后继邻居节点，即该节点指向的节点。\n",
    "        neighbors = list(graph.neighbors(node))\n",
    "        # 对当前节点的邻居节点进行遍历\n",
    "        for neighbor in neighbors:\n",
    "            # 邻居节点中存在 感染者(I)，则该节点有概率被感染为 感染者(I)\n",
    "            if graph.nodes[neighbor]['status'] == 'I':\n",
    "                p = random.random()\n",
    "                if p < beta:\n",
    "                    graph.nodes[node]['status1'] = 'I'\n",
    "                    break\n",
    "    if graph.nodes[node]['status'] == 'I':\n",
    "        p = random.random()\n",
    "        if p < gamma:\n",
    "            graph.nodes[node]['status1'] = 'R'\n",
    "def update(graph):\n",
    "    for node in graph:\n",
    "        graph.nodes[node]['status'] = graph.nodes[node]['status1']\n",
    "\n",
    "def count_node(graph):\n",
    "    \"\"\"\n",
    "    计算当前图内各个状态节点的数目\n",
    "    :param graph: 输入图\n",
    "    :return: 各个状态（S、I、R）的节点数目\n",
    "    \"\"\"\n",
    "    s_num, i_num, r_num = 0, 0, 0\n",
    "    for node in graph:\n",
    "        if graph.nodes[node]['status'] == 'S':\n",
    "            s_num += 1\n",
    "        elif graph.nodes[node]['status'] == 'I':\n",
    "            i_num += 1\n",
    "        else:\n",
    "            r_num += 1\n",
    "    return s_num, i_num, r_num\n",
    "\n",
    "\n",
    "def SIR_network(graph, source, beta, gamma):\n",
    "    \"\"\"\n",
    "    获得感染源的节点序列的SIR感染情况\n",
    "    :param graph: networkx创建的网络\n",
    "    :param source: 需要被设置为感染源的节点Id所构成的序列\n",
    "    :param beta: 感染率\n",
    "    :param gamma: 免疫率\n",
    "    :param step: 迭代次数\n",
    "    \"\"\"\n",
    "    n = graph.number_of_nodes()  # 网络节点个数\n",
    "    sir_values = []  # 存储每一次迭代后网络中感染节点数I+免疫节点数R的总和\n",
    "    # 初始化节点状态\n",
    "    for node in graph:\n",
    "        graph.nodes[node]['status'] = 'S'\n",
    "        graph.nodes[node]['status1'] = 'S'# 将所有节点的状态设置为 易感者（S）\n",
    "    # 设置初始感染源\n",
    "    \n",
    "    graph.nodes[source]['status'] = 'I' \n",
    "    graph.nodes[source]['status1'] = 'I'# 将感染源序列中的节点设置为感染源，状态设置为 感染者（I）\n",
    "    # 记录初始状态\n",
    "    sir_values.append(1 / n)\n",
    "    # 开始迭代感染\n",
    "    s0=n-1\n",
    "    i0=1\n",
    "    r0=0\n",
    "    while 1:\n",
    "        # 针对对每个节点进行状态更新以完成本次迭代\n",
    "        for node in graph:\n",
    "            update_node_status(graph, node, beta, gamma)  # 针对node号节点进行SIR过程\n",
    "        update(graph)\n",
    "        s, i, r = count_node(graph)  # 得到本次迭代结束后各个状态（S、I、R）的节点数目\n",
    "        if s==s0 and i==i0 and r==r0:\n",
    "            break\n",
    "        else:\n",
    "            s0=s\n",
    "            i0=i\n",
    "            r0=r\n",
    "        sir = (i + r) / n  # 该节点的sir值为迭代结束后 感染节点数i+免疫节点数r\n",
    "        sir_values.append(sir)  # 将本次迭代的sir值加入数组\n",
    "    return i0+r0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate(adj):\n",
    "    degree=np.zeros(adj.shape[0])\n",
    "    for i in range(adj.shape[0]):\n",
    "        degree[i]=adj[i].sum()\n",
    "    \n",
    "    sum_1=0\n",
    "    sum_2=0\n",
    "    for i in range(adj.shape[0]):\n",
    "        sum_1+=degree[i]\n",
    "        sum_2+=degree[i]*degree[i]\n",
    "    avg1=sum_1/adj.shape[0]\n",
    "    avg2=sum_2/adj.shape[0]\n",
    "    \n",
    "    return avg1/(avg2-avg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def caculate_avg(graph,node):\n",
    "    sum=0\n",
    "    for i in range(100):\n",
    "        sum+=SIR_network(graph,node,beta,1)\n",
    "    return sum/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj=np.array(adj_Stelzl)\n",
    "beta=caculate(adj)*1.5\n",
    "print(beta)\n",
    "graph = nx.from_numpy_matrix(adj) \n",
    "\n",
    "lable_Stelzl=[caculate_avg(graph,i) for i in range(adj.shape[0])]\n",
    "np.save('lable_Stelzl.npy',lable_Stelzl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj=np.array(adj_vidal)\n",
    "beta=caculate(adj)*1.5\n",
    "print(beta)\n",
    "graph = nx.from_numpy_matrix(adj) \n",
    "\n",
    "lable_vidal=[caculate_avg(graph,i) for i in range(adj.shape[0])]\n",
    "np.save('lable_vidal.npy',lable_vidal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj=np.array(adj_figeys)\n",
    "beta=caculate(adj)*1.5\n",
    "print(beta)\n",
    "graph = nx.from_numpy_matrix(adj) \n",
    "\n",
    "lable_figeys=[caculate_avg(graph,i) for i in range(adj.shape[0])]\n",
    "np.save('lable_figeys.npy',lable_figeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj=np.array(adj_Caenor)\n",
    "beta=caculate(adj)*1.5\n",
    "print(beta)\n",
    "graph = nx.from_numpy_matrix(adj) \n",
    "\n",
    "lable_Caenor=[caculate_avg(graph,i) for i in range(adj.shape[0])]\n",
    "np.save('lable_Caenor.npy',lable_Caenor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj=np.array(adj_powergrid)\n",
    "beta=caculate(adj)*1.5\n",
    "print(beta)\n",
    "graph = nx.from_numpy_matrix(adj) \n",
    "\n",
    "lable_powergrid=[caculate_avg(graph,i) for i in range(adj.shape[0])]\n",
    "np.save('lable_powergrid.npy',lable_powergrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adj=np.array(adj_twitter)\n",
    "beta=caculate(adj)*1.5\n",
    "print(beta)\n",
    "graph = nx.from_numpy_matrix(adj) \n",
    "\n",
    "lable_twitter=[caculate_avg(graph,i) for i in range(adj.shape[0])]\n",
    "np.save('lable_twitter.npy',lable_twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj=np.array(adj_erdos)`\n",
    "beta=caculate(adj)*1.5\n",
    "print(beta)\n",
    "graph = nx.from_numpy_matrix(adj) \n",
    "\n",
    "lable_erdos=[caculate_avg(graph,i) for i in range(adj.shape[0])]\n",
    "np.save('lable_erdos.npy',lable_erdos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj=np.array(adj_usa)\n",
    "beta=caculate(adj)*1.5\n",
    "print(beta)\n",
    "graph = nx.from_numpy_matrix(adj) \n",
    "\n",
    "lable_usa=[caculate_avg(graph,i) for i in range(adj.shape[0])]\n",
    "np.save('lable_usa.npy',lable_usa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj=np.array(adj_oz)\n",
    "beta=caculate(adj)*1.5\n",
    "print(beta)\n",
    "graph = nx.from_numpy_matrix(adj) \n",
    "\n",
    "lable_oz=[caculate_avg(graph,i) for i in range(adj.shape[0])]\n",
    "np.save('lable_oz.npy',lable_oz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj=np.array(adj_ftrust)\n",
    "beta=caculate(adj)*1.5\n",
    "print(beta)\n",
    "graph = nx.from_numpy_matrix(adj) \n",
    "\n",
    "lable_ftrust=[caculate_avg(graph,i) for i in range(adj.shape[0])]\n",
    "np.save('lable_ftrust',lable_ftrust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj=np.array(adj_bible)\n",
    "beta=caculate(adj)*1.5\n",
    "print(beta)\n",
    "graph = nx.from_numpy_matrix(adj) \n",
    "\n",
    "lable_bible=[caculate_avg(graph,i) for i in range(adj.shape[0])]\n",
    "np.save('lable_bible',lable_bible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_jazz=[caculate_avg(graph,i) for i in range(adj_jazz.shape[0])]\n",
    "np.save('lable_jazz.npy',lable_jazz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lable_jazz=np.load(\"lable_jazz.npy\")\n",
    "lable_jazz_t=torch.tensor(lable_jazz).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_twitter=np.load(\"lable_twitter.npy\")\n",
    "lable_twitter_t=torch.tensor(lable_twitter).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_ftrust=np.load(\"lable_ftrust.npy\")\n",
    "lable_ftrust_t=torch.tensor(lable_ftrust).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_erdos=np.load(\"lable_erdos.npy\")\n",
    "lable_erdos_t=torch.tensor(lable_erdos).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_email_2=np.load(\"lable_email_2.npy\")\n",
    "lable_email_2_t=torch.tensor(lable_email_2).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_email=np.load(\"lable_email.npy\")\n",
    "lable_email_t=torch.tensor(lable_email).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_BA=np.load(\"lable_BA_1000_4.npy\")\n",
    "lable_BA_t=torch.tensor(lable_BA).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_cora=np.load(\"lable.npy\")\n",
    "lable_cora_t=torch.tensor(lable_cora).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_usa=np.load(\"lable_usa.npy\")\n",
    "lable_usa_t=torch.tensor(lable_usa).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_oz=np.load(\"lable_oz.npy\")\n",
    "lable_oz_t=torch.tensor(lable_oz).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Den(torch.nn.Module):\n",
    "    def __init__(self,n):\n",
    "        super(Den,self).__init__()   \n",
    "        self.w1 = nn.Parameter(torch.empty(size=(n,6)))\n",
    "        self.w2= nn.Parameter(torch.empty(size=(6,n)))\n",
    "        self.m=torch.nn.Softmax(dim=1)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        #nn.init.xavier_uniform_(self.w.data,gain=1.414)\n",
    "        #nn.init.xavier_uniform_(self.a.data,gain=1.414)\n",
    "        for param in self.parameters():\n",
    "             nn.init.xavier_uniform_(param)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        y=torch.mm(x,self.w1)\n",
    "        x=torch.mm(y,self.w2)\n",
    "        x=self.m(x)\n",
    "        \n",
    "        return y,x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self,input_feature,output_feature):\n",
    "        super(GNN,self).__init__()\n",
    "        self.w = nn.Parameter(torch.empty(size=(input_feature,output_feature)))\n",
    "        self.a= nn.Parameter(torch.empty(size=(1,output_feature)))\n",
    "        self.sigmod=torch.nn.Sigmoid()\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        #nn.init.xavier_uniform_(self.w.data,gain=1.414)\n",
    "        #nn.init.xavier_uniform_(self.a.data,gain=1.414)\n",
    "        for param in self.parameters():\n",
    "             nn.init.xavier_uniform_(param)\n",
    "        \n",
    "    def forward(self,x,adj):\n",
    "        #adj=torch.Tensor(adj.numpy()+np.identity(adj.shape[0]))\n",
    "        adj=torch.FloatTensor(normalize_adj(sp.csr_matrix(adj)+ sp.eye(adj.shape[0])).todense())\n",
    "        x=torch.mm(adj,x)\n",
    "        x=torch.mm(x,self.w)\n",
    "       # x=x.add(self.a)\n",
    "        x=torch.relu(x)\n",
    "        return x\n",
    "    \n",
    "class GAE(nn.Module): #编码器\n",
    "    def __init__(self, n_total_features, n_latent,p_drop=0.):\n",
    "        super(GAE, self).__init__()\n",
    "        self.n_total_features = n_total_features\n",
    "        self.conv1 = GNN(self.n_total_features, 256)\n",
    "       \n",
    "        self.conv2 =GNN(128, 24)\n",
    "       \n",
    "        self.conv3 = GNN(256, n_latent)\n",
    "        self.conv4=GNN(n_latent,self.n_total_features)\n",
    "        self.fc1=torch.nn.Linear(n_latent,256)\n",
    "        self.fc2=torch.nn.Linear(256,1)\n",
    "\n",
    "    def forward(self,x,adj): #实践中一般采取多层的GCN来编码\n",
    "        \n",
    "        x = self.conv1(x, adj)\n",
    "        #x = self.conv2(x, adj)\n",
    "        x = self.conv3(x, adj) #经过三层GCN后得到节点的表示\n",
    "        A = self.fc1(x) #直接算点积'\n",
    "        A=self.fc2(A)\n",
    "        #A = torch.sigmoid(A)\n",
    "        return x,A\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    x,A=model(torch.tensor(np.identity(adj_BA.shape[0])).float(),adj_BA)\n",
    "    \n",
    "    loss_train =torch.norm(A-adj_BA.sum(dim=1).reshape(-1,1), p='fro')\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    x,A=model(torch.tensor(np.identity(adj_BA.shape[0])).float(),adj_BA)\n",
    "   \n",
    "    \n",
    "    loss_val =torch.norm(A-adj_BA.sum(dim=1).reshape(-1,1), p='fro')\n",
    "    \n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.data.item()),\n",
    "          \n",
    "          'loss_val: {}'.format(loss_val.data.item()),\n",
    "        \n",
    "          'time: {}s'.format(time.time() - t))\n",
    "    return loss_train.data.item()\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(node_feature,adj)\n",
    "    loss_test =torch.nn.functional.mse_loss(output, label_t)\n",
    "  \n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {}\".format(loss_test.data.item()),\n",
    "          )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    random.seed(17)\n",
    "    np.random.seed(17)\n",
    "    torch.manual_seed(17)\n",
    "    \n",
    "    model=GAE(adj_BA.shape[0],48)\n",
    "    optimizer = optim.Adam(model.parameters(),lr= 0.01,weight_decay=5e-4)\n",
    "    \n",
    "    t_total = time.time()\n",
    "    loss_values = []\n",
    "    bad_counter = 0\n",
    "    best = 1000+1\n",
    "    best_epoch = 0\n",
    "\n",
    "\n",
    "    for epoch in range(500):\n",
    "        loss_values.append(train(epoch))\n",
    "        if loss_values[-1] < best:\n",
    "            best = loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "        \n",
    "        if bad_counter == 100:\n",
    "            bad_counter += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {}s\".format(time.time() - t_total))\n",
    "    #compute_test()\n",
    "    model.eval()\n",
    "    node_feature_BA,A=model(torch.tensor(np.identity(adj_BA.shape[0])).float(),adj_BA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    x,A=model(torch.tensor(np.identity(adj_erdos.shape[0])).float(),adj_erdos)\n",
    "    \n",
    "    loss_train =torch.norm(A-adj_erdos.sum(dim=1).reshape(-1,1), p='fro')\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    x,A=model(torch.tensor(np.identity(adj_erdos.shape[0])).float(),adj_erdos)\n",
    "   \n",
    "    \n",
    "    loss_val =torch.norm(A-adj_erdos.sum(dim=1).reshape(-1,1), p='fro')\n",
    "    \n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.data.item()),\n",
    "          \n",
    "          'loss_val: {}'.format(loss_val.data.item()),\n",
    "        \n",
    "          'time: {}s'.format(time.time() - t))\n",
    "    return loss_train.data.item()\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(node_feature,adj)\n",
    "    loss_test =torch.nn.functional.mse_loss(output, label_t)\n",
    "  \n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {}\".format(loss_test.data.item()),\n",
    "          )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    random.seed(17)\n",
    "    np.random.seed(17)\n",
    "    torch.manual_seed(17)\n",
    "    \n",
    "    model=GAE(adj_erdos.shape[0],48)\n",
    "    optimizer = optim.Adam(model.parameters(),lr= 0.01,weight_decay=5e-4)\n",
    "    \n",
    "    t_total = time.time()\n",
    "    loss_values = []\n",
    "    bad_counter = 0\n",
    "    best = 1000+1\n",
    "    best_epoch = 0\n",
    "\n",
    "\n",
    "    for epoch in range(100):\n",
    "        loss_values.append(train(epoch))\n",
    "        if loss_values[-1] < best:\n",
    "            best = loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "        \n",
    "        if bad_counter == 100:\n",
    "            bad_counter += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {}s\".format(time.time() - t_total))\n",
    "    #compute_test()\n",
    "    model.eval()\n",
    "    node_feature_erdos,A=model(torch.tensor(np.identity(adj_erdos.shape[0])).float(),adj_erdos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    x,A=model(torch.tensor(np.identity(adj_twitter.shape[0])).float(),adj_twitter)\n",
    "    \n",
    "    loss_train =torch.norm(A-adj_twitter.sum(dim=1).reshape(-1,1), p='fro')\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    x,A=model(torch.tensor(np.identity(adj_twitter.shape[0])).float(),adj_twitter)\n",
    "   \n",
    "    \n",
    "    loss_val =torch.norm(A-adj_twitter.sum(dim=1).reshape(-1,1), p='fro')\n",
    "    \n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.data.item()),\n",
    "          \n",
    "          'loss_val: {}'.format(loss_val.data.item()),\n",
    "        \n",
    "          'time: {}s'.format(time.time() - t))\n",
    "    return loss_train.data.item()\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(node_feature,adj)\n",
    "    loss_test =torch.nn.functional.mse_loss(output, label_t)\n",
    "  \n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {}\".format(loss_test.data.item()),\n",
    "          )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    random.seed(17)\n",
    "    np.random.seed(17)\n",
    "    torch.manual_seed(17)\n",
    "    \n",
    "    model=GAE(adj_twitter.shape[0],48)\n",
    "    optimizer = optim.Adam(model.parameters(),lr= 0.01,weight_decay=5e-4)\n",
    "    \n",
    "    t_total = time.time()\n",
    "    loss_values = []\n",
    "    bad_counter = 0\n",
    "    best = 1000+1\n",
    "    best_epoch = 0\n",
    "\n",
    "\n",
    "    for epoch in range(300):\n",
    "        loss_values.append(train(epoch))\n",
    "        if loss_values[-1] < best:\n",
    "            best = loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "        \n",
    "        if bad_counter == 100:\n",
    "            bad_counter += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {}s\".format(time.time() - t_total))\n",
    "    #compute_test()\n",
    "    model.eval()\n",
    "    node_feature_twitter,A=model(torch.tensor(np.identity(adj_twitter.shape[0])).float(),adj_twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    x,A=model(torch.tensor(np.identity(adj_ftrust.shape[0])).float(),adj_ftrust)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    loss_train =torch.norm(A-adj_ftrust.sum(dim=1).reshape(-1,1), p='fro')\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    x,A=model(torch.tensor(np.identity(adj_ftrust.shape[0])).float(),adj_ftrust)\n",
    "   \n",
    "    \n",
    "    loss_val =  loss_train\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.data.item()),\n",
    "          \n",
    "          'loss_val: {}'.format(loss_val.data.item()),\n",
    "        \n",
    "          'time: {}s'.format(time.time() - t))\n",
    "    return loss_train.data.item()\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(node_feature,adj)\n",
    "    loss_test =torch.nn.functional.mse_loss(output, label_t)\n",
    "  \n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {}\".format(loss_test.data.item()),\n",
    "          )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    random.seed(17)\n",
    "    np.random.seed(17)\n",
    "    torch.manual_seed(17)\n",
    "    \n",
    "    model=GAE(adj_ftrust.shape[0],48)\n",
    "    optimizer = optim.Adam(model.parameters(),lr= 0.01,weight_decay=5e-4)\n",
    "    \n",
    "    t_total = time.time()\n",
    "    loss_values = []\n",
    "    bad_counter = 0\n",
    "    best = 1000+1\n",
    "    best_epoch = 0\n",
    "\n",
    "\n",
    "    for epoch in range(500):\n",
    "        loss_values.append(train(epoch))\n",
    "        if loss_values[-1] < best:\n",
    "            best = loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "        \n",
    "        if bad_counter == 100:\n",
    "            bad_counter += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {}s\".format(time.time() - t_total))\n",
    "    #compute_test()\n",
    "    model.eval()\n",
    "    node_feature_ftrust,A=model(torch.tensor(np.identity(adj_ftrust.shape[0])).float(),adj_ftrust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    x,A=model(torch.tensor(np.identity(adj_cora.shape[0])).float(),adj_cora)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    loss_train =torch.norm(A-adj_cora.sum(dim=1).reshape(-1,1), p='fro')\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    x,A=model(torch.tensor(np.identity(adj_cora.shape[0])).float(),adj_cora)\n",
    "   \n",
    "    \n",
    "    loss_val =  loss_train\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.data.item()),\n",
    "          \n",
    "          'loss_val: {}'.format(loss_val.data.item()),\n",
    "        \n",
    "          'time: {}s'.format(time.time() - t))\n",
    "    return loss_train.data.item()\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(node_feature,adj)\n",
    "    loss_test =torch.nn.functional.mse_loss(output, label_t)\n",
    "  \n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {}\".format(loss_test.data.item()),\n",
    "          )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    random.seed(17)\n",
    "    np.random.seed(17)\n",
    "    torch.manual_seed(17)\n",
    "    \n",
    "    model=GAE(adj_cora.shape[0],48)\n",
    "    optimizer = optim.Adam(model.parameters(),lr= 0.01,weight_decay=5e-4)\n",
    "    \n",
    "    t_total = time.time()\n",
    "    loss_values = []\n",
    "    bad_counter = 0\n",
    "    best = 1000+1\n",
    "    best_epoch = 0\n",
    "\n",
    "\n",
    "    for epoch in range(500):\n",
    "        loss_values.append(train(epoch))\n",
    "        if loss_values[-1] < best:\n",
    "            best = loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "        \n",
    "        if bad_counter == 100:\n",
    "            bad_counter += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {}s\".format(time.time() - t_total))\n",
    "    #compute_test()\n",
    "    model.eval()\n",
    "    node_feature_cora,A=model(torch.tensor(np.identity(adj_cora.shape[0])).float(),adj_cora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    x,A=model(torch.tensor(np.identity(adj_email.shape[0])).float(),adj_email)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    loss_train =torch.norm(A-adj_email.sum(dim=1).reshape(-1,1), p='fro')\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    x,A=model(torch.tensor(np.identity(adj_email.shape[0])).float(),adj_email)\n",
    "   \n",
    "    \n",
    "    loss_val = loss_train\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.data.item()),\n",
    "          \n",
    "          'loss_val: {}'.format(loss_val.data.item()),\n",
    "        \n",
    "          'time: {}s'.format(time.time() - t))\n",
    "    return loss_train.data.item()\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(node_feature,adj)\n",
    "    loss_test =torch.nn.functional.mse_loss(output, label_t)\n",
    "  \n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {}\".format(loss_test.data.item()),\n",
    "          )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    random.seed(17)\n",
    "    np.random.seed(17)\n",
    "    torch.manual_seed(17)\n",
    "    \n",
    "    model=GAE(adj_email.shape[0],48)\n",
    "    optimizer = optim.Adam(model.parameters(),lr= 0.01,weight_decay=5e-4)\n",
    "    \n",
    "    t_total = time.time()\n",
    "    loss_values = []\n",
    "    bad_counter = 0\n",
    "    best = 1000+1\n",
    "    best_epoch = 0\n",
    "\n",
    "\n",
    "    for epoch in range(400):\n",
    "        loss_values.append(train(epoch))\n",
    "        if loss_values[-1] < best:\n",
    "            best = loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "        \n",
    "        if bad_counter == 100:\n",
    "            bad_counter += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {}s\".format(time.time() - t_total))\n",
    "    #compute_test()\n",
    "    model.eval()\n",
    "    node_feature_email,A=model(torch.tensor(np.identity(adj_email.shape[0])).float(),adj_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    x,A=model(torch.tensor(np.identity(adj_email_2.shape[0])).float(),adj_email_2)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    loss_train =torch.norm(A-adj_email_2.sum(dim=1).reshape(-1,1), p='fro')\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    x,A=model(torch.tensor(np.identity(adj_email_2.shape[0])).float(),adj_email_2)\n",
    "   \n",
    "    \n",
    "    loss_val = loss_train\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.data.item()),\n",
    "          \n",
    "          'loss_val: {}'.format(loss_val.data.item()),\n",
    "        \n",
    "          'time: {}s'.format(time.time() - t))\n",
    "    return loss_train.data.item()\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(node_feature,adj)\n",
    "    loss_test =torch.nn.functional.mse_loss(output, label_t)\n",
    "  \n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {}\".format(loss_test.data.item()),\n",
    "          )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    random.seed(17)\n",
    "    np.random.seed(17)\n",
    "    torch.manual_seed(17)\n",
    "    \n",
    "    model=GAE(adj_email_2.shape[0],48)\n",
    "    optimizer = optim.Adam(model.parameters(),lr= 0.01,weight_decay=5e-4)\n",
    "    \n",
    "    t_total = time.time()\n",
    "    loss_values = []\n",
    "    bad_counter = 0\n",
    "    best = 1000+1\n",
    "    best_epoch = 0\n",
    "\n",
    "\n",
    "    for epoch in range(700):\n",
    "        loss_values.append(train(epoch))\n",
    "        if loss_values[-1] < best:\n",
    "            best = loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "        \n",
    "        if bad_counter == 100:\n",
    "            bad_counter += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {}s\".format(time.time() - t_total))\n",
    "    #compute_test()\n",
    "    model.eval()\n",
    "    node_feature_email_2,A=model(torch.tensor(np.identity(adj_email_2.shape[0])).float(),adj_email_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    x,A=model(torch.tensor(np.identity(adj_jazz.shape[0])).float(),adj_jazz)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    loss_train =torch.norm(A-adj_jazz.sum(dim=1).reshape(-1,1), p='fro')\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    x,A=model(torch.tensor(np.identity(adj_jazz.shape[0])).float(),adj_jazz)\n",
    "   \n",
    "    \n",
    "    loss_val = loss_train\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.data.item()),\n",
    "          \n",
    "          'loss_val: {}'.format(loss_val.data.item()),\n",
    "        \n",
    "          'time: {}s'.format(time.time() - t))\n",
    "    return loss_train.data.item()\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(node_feature,adj)\n",
    "    loss_test =torch.nn.functional.mse_loss(output, label_t)\n",
    "  \n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {}\".format(loss_test.data.item()),\n",
    "          )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    random.seed(17)\n",
    "    np.random.seed(17)\n",
    "    torch.manual_seed(17)\n",
    "    \n",
    "    model=GAE(adj_jazz.shape[0],48)\n",
    "    optimizer = optim.Adam(model.parameters(),lr= 0.01,weight_decay=5e-4)\n",
    "    \n",
    "    t_total = time.time()\n",
    "    loss_values = []\n",
    "    bad_counter = 0\n",
    "    best = 1000+1\n",
    "    best_epoch = 0\n",
    "\n",
    "\n",
    "    for epoch in range(700):\n",
    "        loss_values.append(train(epoch))\n",
    "        if loss_values[-1] < best:\n",
    "            best = loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "        \n",
    "        if bad_counter == 100:\n",
    "            bad_counter += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {}s\".format(time.time() - t_total))\n",
    "    #compute_test()\n",
    "    model.eval()\n",
    "    node_feature_jazz,A=model(torch.tensor(np.identity(adj_jazz.shape[0])).float(),adj_jazz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    x,A=model(torch.tensor(np.identity(adj_usa.shape[0])).float(),adj_usa)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    loss_train =torch.norm(A-adj_usa.sum(dim=1).reshape(-1,1), p='fro')\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    x,A=model(torch.tensor(np.identity(adj_usa.shape[0])).float(),adj_usa)\n",
    "   \n",
    "    \n",
    "    loss_val = loss_train\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.data.item()),\n",
    "          \n",
    "          'loss_val: {}'.format(loss_val.data.item()),\n",
    "        \n",
    "          'time: {}s'.format(time.time() - t))\n",
    "    return loss_train.data.item()\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(node_feature,adj)\n",
    "    loss_test =torch.nn.functional.mse_loss(output, label_t)\n",
    "  \n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {}\".format(loss_test.data.item()),\n",
    "          )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    random.seed(17)\n",
    "    np.random.seed(17)\n",
    "    torch.manual_seed(17)\n",
    "    \n",
    "    model=GAE(adj_usa.shape[0],48)\n",
    "    optimizer = optim.Adam(model.parameters(),lr= 0.01,weight_decay=5e-4)\n",
    "    \n",
    "    t_total = time.time()\n",
    "    loss_values = []\n",
    "    bad_counter = 0\n",
    "    best = 1000+1\n",
    "    best_epoch = 0\n",
    "\n",
    "\n",
    "    for epoch in range(500):\n",
    "        loss_values.append(train(epoch))\n",
    "        if loss_values[-1] < best:\n",
    "            best = loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "        \n",
    "        if bad_counter == 100:\n",
    "            bad_counter += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {}s\".format(time.time() - t_total))\n",
    "    #compute_test()\n",
    "    model.eval()\n",
    "    node_feature_usa,A=model(torch.tensor(np.identity(adj_usa.shape[0])).float(),adj_usa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_oz=torch.tensor(adj_oz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    x,A=model(torch.tensor(np.identity(adj_oz.shape[0])).float(),adj_oz)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    loss_train =torch.norm(A-adj_oz.sum(dim=1).reshape(-1,1), p='fro')\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    x,A=model(torch.tensor(np.identity(adj_oz.shape[0])).float(),adj_oz)\n",
    "   \n",
    "    \n",
    "    loss_val = loss_train\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.data.item()),\n",
    "          \n",
    "          'loss_val: {}'.format(loss_val.data.item()),\n",
    "        \n",
    "          'time: {}s'.format(time.time() - t))\n",
    "    return loss_train.data.item()\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(node_feature,adj)\n",
    "    loss_test =torch.nn.functional.mse_loss(output, label_t)\n",
    "  \n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {}\".format(loss_test.data.item()),\n",
    "          )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    random.seed(17)\n",
    "    np.random.seed(17)\n",
    "    torch.manual_seed(17)\n",
    "    \n",
    "    model=GAE(adj_oz.shape[0],48)\n",
    "    optimizer = optim.Adam(model.parameters(),lr= 0.01,weight_decay=5e-4)\n",
    "    \n",
    "    t_total = time.time()\n",
    "    loss_values = []\n",
    "    bad_counter = 0\n",
    "    best = 1000+1\n",
    "    best_epoch = 0\n",
    "\n",
    "\n",
    "    for epoch in range(500):\n",
    "        loss_values.append(train(epoch))\n",
    "        if loss_values[-1] < best:\n",
    "            best = loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "        \n",
    "        if bad_counter == 100:\n",
    "            bad_counter += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {}s\".format(time.time() - t_total))\n",
    "    #compute_test()\n",
    "    model.eval()\n",
    "    node_feature_oz,A=model(torch.tensor(np.identity(adj_oz.shape[0])).float(),adj_oz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    x,A=model(torch.tensor(np.identity(adj_Stelzl.shape[0])).float(),adj_Stelzl)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    loss_train =torch.norm(A-adj_Stelzl.sum(dim=1).reshape(-1,1), p='fro')\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    x,A=model(torch.tensor(np.identity(adj_Stelzl.shape[0])).float(),adj_Stelzl)\n",
    "   \n",
    "    \n",
    "    loss_val = loss_train\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.data.item()),\n",
    "          \n",
    "          'loss_val: {}'.format(loss_val.data.item()),\n",
    "        \n",
    "          'time: {}s'.format(time.time() - t))\n",
    "    return loss_train.data.item()\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(node_feature,adj)\n",
    "    loss_test =torch.nn.functional.mse_loss(output, label_t)\n",
    "  \n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {}\".format(loss_test.data.item()),\n",
    "          )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    random.seed(17)\n",
    "    np.random.seed(17)\n",
    "    torch.manual_seed(17)\n",
    "    \n",
    "    model=GAE(adj_Stelzl.shape[0],48)\n",
    "    optimizer = optim.Adam(model.parameters(),lr= 0.01,weight_decay=5e-4)\n",
    "    \n",
    "    t_total = time.time()\n",
    "    loss_values = []\n",
    "    bad_counter = 0\n",
    "    best = 1000+1\n",
    "    best_epoch = 0\n",
    "\n",
    "\n",
    "    for epoch in range(500):\n",
    "        loss_values.append(train(epoch))\n",
    "        if loss_values[-1] < best:\n",
    "            best = loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "        \n",
    "        if bad_counter == 100:\n",
    "            bad_counter += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {}s\".format(time.time() - t_total))\n",
    "    #compute_test()\n",
    "    model.eval()\n",
    "    node_feature_Stelzl,A=model(torch.tensor(np.identity(adj_Stelzl.shape[0])).float(),adj_Stelzl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    x,A=model(torch.tensor(np.identity(adj_figeys.shape[0])).float(),adj_figeys)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    loss_train =torch.norm(A-adj_figeys.sum(dim=1).reshape(-1,1), p='fro')\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    x,A=model(torch.tensor(np.identity(adj_figeys.shape[0])).float(),adj_figeys)\n",
    "   \n",
    "    \n",
    "    loss_val = loss_train\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.data.item()),\n",
    "          \n",
    "          'loss_val: {}'.format(loss_val.data.item()),\n",
    "        \n",
    "          'time: {}s'.format(time.time() - t))\n",
    "    return loss_train.data.item()\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(node_feature,adj)\n",
    "    loss_test =torch.nn.functional.mse_loss(output, label_t)\n",
    "  \n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {}\".format(loss_test.data.item()),\n",
    "          )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    random.seed(17)\n",
    "    np.random.seed(17)\n",
    "    torch.manual_seed(17)\n",
    "    \n",
    "    model=GAE(adj_figeys.shape[0],48)\n",
    "    optimizer = optim.Adam(model.parameters(),lr= 0.01,weight_decay=5e-4)\n",
    "    \n",
    "    t_total = time.time()\n",
    "    loss_values = []\n",
    "    bad_counter = 0\n",
    "    best = 1000+1\n",
    "    best_epoch = 0\n",
    "\n",
    "\n",
    "    for epoch in range(500):\n",
    "        loss_values.append(train(epoch))\n",
    "        if loss_values[-1] < best:\n",
    "            best = loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "        \n",
    "        if bad_counter == 100:\n",
    "            bad_counter += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {}s\".format(time.time() - t_total))\n",
    "    #compute_test()\n",
    "    model.eval()\n",
    "    node_feature_figeys,A=model(torch.tensor(np.identity(adj_figeys.shape[0])).float(),adj_figeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    x,A=model(torch.tensor(np.identity(adj_vidal.shape[0])).float(),adj_vidal)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    loss_train =torch.norm(A-adj_vidal.sum(dim=1).reshape(-1,1), p='fro')\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    x,A=model(torch.tensor(np.identity(adj_vidal.shape[0])).float(),adj_vidal)\n",
    "   \n",
    "    \n",
    "    loss_val = loss_train\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.data.item()),\n",
    "          \n",
    "          'loss_val: {}'.format(loss_val.data.item()),\n",
    "        \n",
    "          'time: {}s'.format(time.time() - t))\n",
    "    return loss_train.data.item()\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(node_feature,adj)\n",
    "    loss_test =torch.nn.functional.mse_loss(output, label_t)\n",
    "  \n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {}\".format(loss_test.data.item()),\n",
    "          )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    random.seed(17)\n",
    "    np.random.seed(17)\n",
    "    torch.manual_seed(17)\n",
    "    \n",
    "    model=GAE(adj_vidal.shape[0],48)\n",
    "    optimizer = optim.Adam(model.parameters(),lr= 0.01,weight_decay=5e-4)\n",
    "    \n",
    "    t_total = time.time()\n",
    "    loss_values = []\n",
    "    bad_counter = 0\n",
    "    best = 1000+1\n",
    "    best_epoch = 0\n",
    "\n",
    "\n",
    "    for epoch in range(500):\n",
    "        loss_values.append(train(epoch))\n",
    "        if loss_values[-1] < best:\n",
    "            best = loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "        \n",
    "        if bad_counter == 100:\n",
    "            bad_counter += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {}s\".format(time.time() - t_total))\n",
    "    #compute_test()\n",
    "    model.eval()\n",
    "    node_feature_vidal,A=model(torch.tensor(np.identity(adj_vidal.shape[0])).float(),adj_vidal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    x,A=model(torch.tensor(np.identity(adj_Caenor.shape[0])).float(),adj_Caenor)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    loss_train =torch.norm(A-adj_Caenor.sum(dim=1).reshape(-1,1), p='fro')\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    x,A=model(torch.tensor(np.identity(adj_Caenor.shape[0])).float(),adj_Caenor)\n",
    "   \n",
    "    \n",
    "    loss_val = loss_train\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.data.item()),\n",
    "          \n",
    "          'loss_val: {}'.format(loss_val.data.item()),\n",
    "        \n",
    "          'time: {}s'.format(time.time() - t))\n",
    "    return loss_train.data.item()\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(node_feature,adj)\n",
    "    loss_test =torch.nn.functional.mse_loss(output, label_t)\n",
    "  \n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {}\".format(loss_test.data.item()),\n",
    "          )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    random.seed(17)\n",
    "    np.random.seed(17)\n",
    "    torch.manual_seed(17)\n",
    "    \n",
    "    model=GAE(adj_Caenor.shape[0],48)\n",
    "    optimizer = optim.Adam(model.parameters(),lr= 0.01,weight_decay=5e-4)\n",
    "    \n",
    "    t_total = time.time()\n",
    "    loss_values = []\n",
    "    bad_counter = 0\n",
    "    best = 1000+1\n",
    "    best_epoch = 0\n",
    "\n",
    "\n",
    "    for epoch in range(500):\n",
    "        loss_values.append(train(epoch))\n",
    "        if loss_values[-1] < best:\n",
    "            best = loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "        \n",
    "        if bad_counter == 100:\n",
    "            bad_counter += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {}s\".format(time.time() - t_total))\n",
    "    #compute_test()\n",
    "    model.eval()\n",
    "    node_feature_Caenor,A=model(torch.tensor(np.identity(adj_Caenor.shape[0])).float(),adj_Caenor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    x,A=model(torch.tensor(np.identity(adj_powergrid.shape[0])).float(),adj_powergrid)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    loss_train =torch.norm(A-adj_powergrid.sum(dim=1).reshape(-1,1), p='fro')\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    x,A=model(torch.tensor(np.identity(adj_powergrid.shape[0])).float(),adj_powergrid)\n",
    "   \n",
    "    \n",
    "    loss_val = loss_train\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.data.item()),\n",
    "          \n",
    "          'loss_val: {}'.format(loss_val.data.item()),\n",
    "        \n",
    "          'time: {}s'.format(time.time() - t))\n",
    "    return loss_train.data.item()\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(node_feature,adj)\n",
    "    loss_test =torch.nn.functional.mse_loss(output, label_t)\n",
    "  \n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {}\".format(loss_test.data.item()),\n",
    "          )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    random.seed(17)\n",
    "    np.random.seed(17)\n",
    "    torch.manual_seed(17)\n",
    "    \n",
    "    model=GAE(adj_powergrid.shape[0],48)\n",
    "    optimizer = optim.Adam(model.parameters(),lr= 0.01,weight_decay=5e-4)\n",
    "    \n",
    "    t_total = time.time()\n",
    "    loss_values = []\n",
    "    bad_counter = 0\n",
    "    best = 1000+1\n",
    "    best_epoch = 0\n",
    "\n",
    "\n",
    "    for epoch in range(500):\n",
    "        loss_values.append(train(epoch))\n",
    "        if loss_values[-1] < best:\n",
    "            best = loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "        \n",
    "        if bad_counter == 100:\n",
    "            bad_counter += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {}s\".format(time.time() - t_total))\n",
    "    #compute_test()\n",
    "    model.eval()\n",
    "    node_feature_powergrid,A=model(torch.tensor(np.identity(adj_powergrid.shape[0])).float(),adj_powergrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNnet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNnet,self).__init__()       \n",
    "        self.conv1=torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1,\n",
    "                            out_channels=10,\n",
    "                            kernel_size=(3,3),\n",
    "                            stride=1,\n",
    "                            padding=1),\n",
    "            \n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=(2,2))\n",
    "\n",
    "        )\n",
    "        self.conv2=torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=10,\n",
    "                            out_channels=20,\n",
    "                            kernel_size=(3,3),\n",
    "                            stride=1,\n",
    "                            padding=1),\n",
    "            \n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=(2,2))\n",
    "\n",
    "        )\n",
    "        self.fc=torch.nn.Linear(980,6)\n",
    "     \n",
    "        \n",
    "            \n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=self.fc(x.view(x.size(0),-1))\n",
    "        return x\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self,input_feature,output_feature):\n",
    "        super(GNN,self).__init__()\n",
    "        self.w = nn.Parameter(torch.empty(size=(input_feature,output_feature)))\n",
    "        self.a= nn.Parameter(torch.empty(size=(1,output_feature)))\n",
    "        self.sigmod=torch.nn.Sigmoid()\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        #nn.init.xavier_uniform_(self.w.data,gain=1.414)\n",
    "        #nn.init.xavier_uniform_(self.a.data,gain=1.414)\n",
    "        for param in self.parameters():\n",
    "             nn.init.xavier_uniform_(param)\n",
    "        \n",
    "    def forward(self,x,adj):\n",
    "        adj=torch.Tensor(adj.numpy()+np.identity(adj.shape[0]))\n",
    "        #adj=torch.FloatTensor(normalize_adj(sp.csr_matrix(adj)+ sp.eye(adj.shape[0])).todense())\n",
    "        x=torch.mm(adj,x)\n",
    "        x=torch.mm(x,self.w)\n",
    "        x=x.add(self.a)\n",
    "        x=torch.relu(x)\n",
    "        return x\n",
    "\n",
    "class CGNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CGNN,self).__init__()\n",
    "        self.layer1=CNNnet()\n",
    "        self.layer2=GNN(48,6)\n",
    "        self.layer3=GNN(6,12)\n",
    "        self.fc=torch.nn.Linear(12,1)\n",
    "       \n",
    "     \n",
    "    \n",
    "    \n",
    "    def forward(self,x,adj):\n",
    "        #x=self.layer1(x)\n",
    "        x=self.layer2(x,adj)\n",
    "        x=self.layer3(x,adj)\n",
    "       # x=self.layer4(x,adj)\n",
    "        \n",
    "        x=self.fc(x.view(x.size(0),-1))\n",
    "        x=torch.relu(x)\n",
    "        x=x.flatten()\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    \"\"\"GAT层\"\"\"\n",
    "    def __init__(self,input_feature,output_feature,dropout,alpha,concat=True):\n",
    "        super(GATLayer,self).__init__()\n",
    "        self.input_feature = input_feature\n",
    "        self.output_feature = output_feature\n",
    "        self.alpha = alpha\n",
    "        self.dropout = dropout\n",
    "        self.concat = concat\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*output_feature,1)))\n",
    "        self.w = nn.Parameter(torch.empty(size=(input_feature,output_feature)))\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.w.data,gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data,gain=1.414)\n",
    "    \n",
    "    def forward(self,h,adj):\n",
    "        Wh = torch.mm(h,self.w)\n",
    "        e = self._prepare_attentional_mechanism_input(Wh)\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec) # adj>0的位置使用e对应位置的值替换，其余都为-9e15，这样设定经过Softmax后每个节点对应的行非邻居都会变为0。\n",
    "        attention = F.softmax(attention, dim=1) # 每行做Softmax，相当于每个节点做softmax\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.mm(attention, Wh) # 得到下一层的输入\n",
    "        \n",
    "        if self.concat:\n",
    "            return F.relu(h_prime) #激活\n",
    "        else:\n",
    "            return h_prime\n",
    "        \n",
    "    def _prepare_attentional_mechanism_input(self,Wh):\n",
    "        \n",
    "        Wh1 = torch.matmul(Wh,self.a[:self.output_feature,:]) # N*out_size @ out_size*1 = N*1\n",
    "        \n",
    "        Wh2 = torch.matmul(Wh,self.a[self.output_feature:,:]) # N*1\n",
    "        \n",
    "        e = Wh1+Wh2.T # Wh1的每个原始与Wh2的所有元素相加，生成N*N的矩阵\n",
    "        return self.leakyrelu(e)\n",
    "    \n",
    "class GAT(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,output_size,dropout,alpha,nheads,concat=True):\n",
    "        super(GAT,self).__init__()\n",
    "        self.dropout= dropout\n",
    "        self.attention = [GATLayer(input_size, hidden_size, dropout=dropout, alpha=alpha,concat=True) for _ in range(nheads)]\n",
    "        for i,attention in enumerate(self.attention):\n",
    "            self.add_module('attention_{}'.format(i),attention)\n",
    "        \n",
    "        self.out_att = GATLayer(hidden_size*nheads, output_size, dropout=dropout, alpha=alpha,concat=False)\n",
    "        self.fc=torch.nn.Linear(28,1)\n",
    "        #self.layer2=GNN(28,128)\n",
    "        \n",
    "    def forward(self,x,adj):\n",
    "        #x = F.dropout(x,self.dropout,training=self.training)\n",
    "        #x = torch.cat([att(x,adj) for att in self.attention],dim=1)\n",
    "        #x = F.dropout(x,self.dropout,training=self.training)\n",
    "       # x = self.out_att(x,adj)\n",
    "        #x=self.layer2(x,adj)\n",
    "        x.view(x.size(0),-1)\n",
    "        x=self.fc(x.view(x.size(0),-1))\n",
    "        x=torch.relu(x)\n",
    "        x=x.flatten()\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RCNN,self).__init__()       \n",
    "        self.conv1=torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1,\n",
    "                            out_channels=16,\n",
    "                            kernel_size=(5,5),\n",
    "                            stride=1,\n",
    "                            padding=2),\n",
    "            \n",
    "            torch.nn.LeakyReLU()\n",
    "            \n",
    "\n",
    "        )\n",
    "        self.conv2=torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=16,\n",
    "                            out_channels=32,\n",
    "                            kernel_size=(5,5),\n",
    "                            stride=1,\n",
    "                            padding=2),\n",
    "            \n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            torch.nn.MaxPool2d(kernel_size=(2,2))\n",
    "\n",
    "        )\n",
    "        self.fc=torch.nn.Linear(1568,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "            x=self.conv1(x)\n",
    "            x=self.conv2(x)\n",
    "            x=self.fc(x.view(x.size(0),-1))\n",
    "            x=torch.relu(x)\n",
    "            x=x.flatten()\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_t=lable_BA_t\n",
    "node_feature=node_feature_BA.data\n",
    "adj=adj_BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MI(res):\n",
    "    a=pd.DataFrame(res.detach().numpy())\n",
    "    x=len(a)\n",
    "    a.rank(axis=0,method='min',numeric_only=None,\n",
    "    na_option='keep',ascending=True,pct=False)\n",
    "    b=a.iloc[:,0].value_counts()\n",
    "    y=0\n",
    "    for i in range(len(b)):\n",
    "        y=y+b.iloc[i]*(b.iloc[i]-1)\n",
    "    ans=(1-y/(x*(x-1)))*(1-y/(x*(x-1)))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListMLE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ListMLE, self).__init__()\n",
    "    def forward(self, outputs, labels):\n",
    "        scores = torch.zeros_like(outputs)\n",
    "        for t in range(scores.size()[0]):\n",
    "            scores[t] = torch.logcumsumexp(outputs[t, labels[t]], dim=0)\n",
    "        loss = torch.mean(scores - outputs)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_loss=ListMLE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#DEFAULT_EPS = 1e-10\n",
    "DEFAULT_EPS = 0.0001\n",
    "PADDED_Y_VALUE = -1\n",
    "PADDED_INDEX_VALUE = -1\n",
    "\n",
    "def listMLE(y_pred, y_true, eps=DEFAULT_EPS, padded_value_indicator=PADDED_Y_VALUE):\n",
    "    \"\"\"\n",
    "    FROM: https://github.com/allegro/allRank/blob/master/allrank/models/losses/listMLE.py\n",
    "    ListMLE loss introduced in \"Listwise Approach to Learning to Rank - Theory and Algorithm\".\n",
    "    :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "    :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "    :param eps: epsilon value, used for numerical stability\n",
    "    :param padded_value_indicator: an indicator of the y_true index containing a padded item, e.g. -1\n",
    "    :return: loss value, a torch.Tensor\n",
    "    \"\"\"\n",
    "    # Reshape the input\n",
    "    #if len(y_true.size()) == 1:\n",
    "    y_pred = y_pred.view(1, -1)\n",
    "    y_true = y_true.view(1, -1)\n",
    "    print('listmle: y_true:', y_true.size(), 'y_pred', y_pred.size())\n",
    "    # shuffle for randomised tie resolution\n",
    "    random_indices = range(y_pred.shape[-1])\n",
    "    print(y_pred.shape[-1])\n",
    "    y_pred_shuffled = y_pred[:, random_indices]\n",
    "    y_true_shuffled = y_true[:, random_indices]\n",
    "\n",
    "    y_true_sorted, indices = y_true_shuffled.sort(descending=True, dim=-1)\n",
    "\n",
    "    mask = y_true_sorted == padded_value_indicator\n",
    "\n",
    "    preds_sorted_by_true = torch.gather(y_pred_shuffled, dim=1, index=indices)\n",
    "    preds_sorted_by_true[mask] = float(\"-inf\")\n",
    "\n",
    "    max_pred_values, _ = preds_sorted_by_true.max(dim=1, keepdim=True)\n",
    "\n",
    "    preds_sorted_by_true_minus_max = preds_sorted_by_true - max_pred_values\n",
    "\n",
    "    cumsums = torch.cumsum(preds_sorted_by_true_minus_max.exp().flip(dims=[1]), dim=1).flip(dims=[1])\n",
    "\n",
    "    observation_loss = torch.log(cumsums + eps) - preds_sorted_by_true_minus_max\n",
    "    #print('listmle obs loss:', observation_loss)\n",
    "\n",
    "    observation_loss[mask] = 0.0\n",
    "    listmle = torch.mean(torch.sum(observation_loss, dim=1))\n",
    "    print('listmle loss:', listmle.item())\n",
    "\n",
    "    return listmle\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    output = model(node_feature,adj)\n",
    "    loss_train = listMLE(output, label_t)\n",
    "    #loss_train = torch.nn.functional.mse_loss(output, label_t)\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    output = model(node_feature, adj)\n",
    "   \n",
    "    \n",
    "    loss_val = torch.nn.functional.mse_loss(output, label_t)\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.data.item()),\n",
    "          \n",
    "          'loss_val: {}'.format(loss_val.data.item()),\n",
    "        \n",
    "          'time: {}s'.format(time.time() - t))\n",
    "    return loss_train.data.item()\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(node_feature, adj)\n",
    "    loss_test =torch.nn.functional.mse_loss(output, label_t)\n",
    "  \n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {}\".format(loss_test.data.item()),\n",
    "          )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    random.seed(17)\n",
    "    np.random.seed(17)\n",
    "    torch.manual_seed(17)\n",
    "    \n",
    "    model = CGNN()\n",
    "    optimizer = optim.Adam(model.parameters(),lr=0.001,weight_decay=5e-4)\n",
    "    \n",
    "    t_total = time.time()\n",
    "    loss_values = []\n",
    "    bad_counter = 0\n",
    "    best = 1000+1\n",
    "    best_epoch = 0\n",
    "\n",
    "\n",
    "    for epoch in range(3000):\n",
    "        loss_values.append(train(epoch))\n",
    "        if loss_values[-1] < best:\n",
    "            best = loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "        \n",
    "        if bad_counter == 100:\n",
    "             bad_counter += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {}s\".format(time.time() - t_total))\n",
    "    compute_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_Stelzl=np.load(\"lable_Stelzl.npy\")\n",
    "lable_Stelzl_t=torch.tensor(lable_Stelzl).float()\n",
    "\n",
    "lable_figeys=np.load(\"lable_figeys.npy\")\n",
    "lable_figeys_t=torch.tensor(lable_figeys).float()\n",
    "\n",
    "lable_vidal=np.load(\"lable_vidal.npy\")\n",
    "lable_vidal_t=torch.tensor(lable_vidal).float()\n",
    "\n",
    "lable_Caenor=np.load(\"lable_Caenor.npy\")\n",
    "lable_Caenor_t=torch.tensor(lable_Caenor).float()\n",
    "\n",
    "lable_powergrid=np.load(\"lable_powergrid.npy\")\n",
    "lable_powergrid_t=torch.tensor(lable_powergrid).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cgnn=model\n",
    "model_cgnn.eval()\n",
    "start=time.time()\n",
    "output = model_cgnn(node_feature_email,adj_email)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "output_rank=output.detach().numpy().argsort()\n",
    "\n",
    "#print(output_rank)\n",
    "#print(label_rank)\n",
    "loss_train = torch.nn.functional.mse_loss(output,lable_email_t)\n",
    "print(loss_train)\n",
    "print(kendalltau(output.detach().numpy(),lable_email_t))\n",
    "print(MI(output))\n",
    "#np.save(\"output_agnn_email\",output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cgnn=model\n",
    "model_cgnn.eval()\n",
    "start=time.time()\n",
    "output = model_cgnn(node_feature_email_2,adj_email_2)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "output_rank=output.detach().numpy().argsort()\n",
    "\n",
    "#print(output_rank)\n",
    "#print(label_rank)\n",
    "loss_train = torch.nn.functional.mse_loss(output,lable_email_2_t)\n",
    "print(loss_train)\n",
    "print(kendalltau(output.detach().numpy(),lable_email_2_t))\n",
    "print(MI(output))\n",
    "#np.save(\"output_agnn_email_2\",output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cgnn=model\n",
    "model_cgnn.eval()\n",
    "start=time.time()\n",
    "output = model_cgnn(node_feature_cora,adj_cora)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "output_rank=output.detach().numpy().argsort()\n",
    "\n",
    "#print(output_rank)\n",
    "#print(label_rank)\n",
    "loss_train = torch.nn.functional.mse_loss(output,lable_cora_t)\n",
    "print(loss_train)\n",
    "print(kendalltau(output.detach().numpy(),lable_cora_t))\n",
    "print(MI(output))\n",
    "#np.save(\"output_agnn_cora\",output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cgnn=model\n",
    "model_cgnn.eval()\n",
    "start=time.time()\n",
    "output = model_cgnn(node_feature_jazz,adj_jazz)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "output_rank=output.detach().numpy().argsort()\n",
    "\n",
    "#print(output_rank)\n",
    "#print(label_rank)\n",
    "loss_train = torch.nn.functional.mse_loss(output,lable_jazz_t)\n",
    "print(loss_train)\n",
    "print(kendalltau(output.detach().numpy(),lable_jazz_t))\n",
    "print(MI(output))\n",
    "#np.save(\"output_agnn_jazz\",output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cgnn=model\n",
    "model_cgnn.eval()\n",
    "start=time.time()\n",
    "output = model_cgnn(node_feature_usa,adj_usa)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "output_rank=output.detach().numpy().argsort()\n",
    "\n",
    "#print(output_rank)\n",
    "#print(label_rank)\n",
    "loss_train = torch.nn.functional.mse_loss(output,lable_usa_t)\n",
    "print(loss_train)\n",
    "print(kendalltau(output.detach().numpy(),lable_usa_t))\n",
    "print(MI(output))\n",
    "#np.save(\"output_agnn_usa\",output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cgnn=model\n",
    "model_cgnn.eval()\n",
    "start=time.time()\n",
    "output = model_cgnn(node_feature_oz,adj_oz)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "output_rank=output.detach().numpy().argsort()\n",
    "\n",
    "#print(output_rank)\n",
    "#print(label_rank)\n",
    "loss_train = torch.nn.functional.mse_loss(output,lable_oz_t)\n",
    "print(loss_train)\n",
    "print(kendalltau(output.detach().numpy(),lable_oz_t))\n",
    "print(MI(output))\n",
    "#np.save(\"output_agnn_oz\",output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_cgnn=model\n",
    "model_cgnn.eval()\n",
    "start=time.time()\n",
    "output = model_cgnn(node_feature_Stelzl,adj_Stelzl)\n",
    "end=time.time()\n",
    "output_rank=output.detach().numpy().argsort()\n",
    "\n",
    "#print(output_rank)\n",
    "#print(label_rank)\n",
    "loss_train = torch.nn.functional.mse_loss(output,lable_Stelzl_t)\n",
    "print(loss_train)\n",
    "print(kendalltau(output.detach().numpy(),lable_Stelzl_t))\n",
    "print(MI(output))\n",
    "print(end-start)\n",
    "np.save(\"output_agnn_Stelzl\",output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_cgnn=model\n",
    "model_cgnn.eval()\n",
    "start=time.time()\n",
    "output = model_cgnn(node_feature_figeys,adj_figeys)\n",
    "end=time.time()\n",
    "output_rank=output.detach().numpy().argsort()\n",
    "\n",
    "#print(output_rank)\n",
    "#print(label_rank)\n",
    "loss_train = torch.nn.functional.mse_loss(output,lable_figeys_t)\n",
    "print(loss_train)\n",
    "print(kendalltau(output.detach().numpy(),lable_figeys_t))\n",
    "print(MI(output))\n",
    "print(end-start)\n",
    "np.save(\"output_agnn_figeys\",output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_cgnn=model\n",
    "model_cgnn.eval()\n",
    "start=time.time()\n",
    "output = model_cgnn(node_feature_vidal,adj_vidal)\n",
    "end=time.time()\n",
    "output_rank=output.detach().numpy().argsort()\n",
    "\n",
    "#print(output_rank)\n",
    "#print(label_rank)\n",
    "loss_train = torch.nn.functional.mse_loss(output,lable_vidal_t)\n",
    "print(loss_train)\n",
    "print(kendalltau(output.detach().numpy(),lable_vidal_t))\n",
    "print(MI(output))\n",
    "print(end-start)\n",
    "np.save(\"output_agnn_vidal\",output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_cgnn=model\n",
    "model_cgnn.eval()\n",
    "start=time.time()\n",
    "output = model_cgnn(node_feature_Caenor,adj_Caenor)\n",
    "end=time.time()\n",
    "output_rank=output.detach().numpy().argsort()\n",
    "\n",
    "#print(output_rank)\n",
    "#print(label_rank)\n",
    "loss_train = torch.nn.functional.mse_loss(output,lable_Caenor_t)\n",
    "print(loss_train)\n",
    "print(kendalltau(output.detach().numpy(),lable_Caenor_t))\n",
    "print(MI(output))\n",
    "print(end-start)\n",
    "np.save(\"output_agnn_Caenor\",output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_cgnn=model\n",
    "model_cgnn.eval()\n",
    "start=time.time()\n",
    "output = model_cgnn(node_feature_powergrid,adj_powergrid)\n",
    "end=time.time()\n",
    "output_rank=output.detach().numpy().argsort()\n",
    "\n",
    "#print(output_rank)\n",
    "#print(label_rank)\n",
    "loss_train = torch.nn.functional.mse_loss(output,lable_powergrid_t)\n",
    "print(loss_train)\n",
    "print(kendalltau(output.detach().numpy(),lable_powergrid_t))\n",
    "print(MI(output))\n",
    "print(end-start)\n",
    "np.save(\"output_agnn_powergrid\",output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_cgnn=model\n",
    "model_cgnn.eval()\n",
    "start=time.time()\n",
    "output = model_cgnn(node_feature_twitter,adj_twitter)\n",
    "end=time.time()\n",
    "output_rank=output.detach().numpy().argsort()\n",
    "\n",
    "#print(output_rank)\n",
    "#print(label_rank)\n",
    "loss_train = torch.nn.functional.mse_loss(output,lable_twitter_t)\n",
    "print(loss_train)\n",
    "print(kendalltau(output.detach().numpy(),lable_twitter_t))\n",
    "print(MI(output))\n",
    "print(end-start)\n",
    "#np.save(\"output_agnn_twitter\",output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_cgnn=model\n",
    "model_cgnn.eval()\n",
    "start=time.time()\n",
    "output = model_cgnn(node_feature_erdos,adj_erdos)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "output_rank=output.detach().numpy().argsort()\n",
    "\n",
    "#print(output_rank)\n",
    "#print(label_rank)\n",
    "loss_train = torch.nn.functional.mse_loss(output,lable_erdos_t)\n",
    "print(loss_train)\n",
    "print(kendalltau(output.detach().numpy(),lable_erdos_t))\n",
    "print(MI(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cgnn=model\n",
    "model_cgnn.eval()\n",
    "start=time.time()\n",
    "output = model_cgnn(node_feature_ftrust,adj_ftrust)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "output_rank=output.detach().numpy().argsort()\n",
    "\n",
    "#print(output_rank)\n",
    "#print(label_rank)\n",
    "loss_train = torch.nn.functional.mse_loss(output,lable_ftrust_t)\n",
    "print(loss_train)\n",
    "print(kendalltau(output.detach().numpy(),lable_ftrust_t))\n",
    "print(MI(output))\n",
    "#np.save(\"output_agnn_ftrust\",output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cgnn=model\n",
    "model_cgnn.eval()\n",
    "start=time.time()\n",
    "output = model_cgnn(node_feature_bible,adj_bible)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "output_rank=output.detach().numpy().argsort()\n",
    "\n",
    "#print(output_rank)\n",
    "#print(label_rank)\n",
    "loss_train = torch.nn.functional.mse_loss(output,lable_bible_t)\n",
    "print(loss_train)\n",
    "print(kendalltau(output.detach().numpy(),lable_bible_t))\n",
    "print(MI(output))\n",
    "#np.save(\"output_agnn_bible\",output.detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
